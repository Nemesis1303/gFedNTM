{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8ca2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile as zp\n",
    "from pathlib import Path\n",
    "from gensim.utils import check_output\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax\n",
    "import shutil\n",
    "from subprocess import check_output\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import colored\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914961b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printgr(text):\n",
    "    print(colored.stylize(text, colored.fg('green')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172e1627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/usuarios_ml4ds/lbartolome/2hNTM\n"
     ]
    }
   ],
   "source": [
    "cd /export/usuarios_ml4ds/lbartolome/2hNTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d7a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE  README.md  config.cf.default  \u001b[0m\u001b[01;34mdata\u001b[0m/  main.py  \u001b[00;32mrequirements.txt\u001b[0m  \u001b[01;34msrc\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7e2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"/export/usuarios_ml4ds/lbartolome/2hNTM/data/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba3fa5",
   "metadata": {},
   "source": [
    "## **0. Auxiliary functions**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b4aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickler(file: str):\n",
    "    \"\"\"Unpickle file\"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0155fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateArray(arr, n, d):\n",
    "    temp = []\n",
    "    i = 0\n",
    "    while (i < d):\n",
    "        temp.append(arr[i])\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    while (d < n):\n",
    "        arr[i] = arr[d]\n",
    "        i = i + 1\n",
    "        d = d + 1\n",
    "    arr[:] = arr[: i] + temp\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d11ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSynthetic(just_inf, gen_docs, vocab_size, n_topics, beta, alpha, n_docs,\n",
    "                      n_docs_inf, n_docs_global_inf, nwords, alg, n_nodes,\n",
    "                      frozen_topics, prior_frozen, own_topics, prior_nofrozen):\n",
    "    \n",
    "    if just_inf:\n",
    "        n_total_docs = n_docs_global_inf\n",
    "    else:\n",
    "        n_total_docs = n_docs + n_docs_inf\n",
    "\n",
    "    # Step 1 - generation of topics\n",
    "    topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
    "    \n",
    "    # Step 2 - generation of document topic proportions\n",
    "    doc_topics_all = []\n",
    "    for i in np.arange(n_nodes):\n",
    "        doc_topics = np.random.dirichlet(prior_frozen + prior_nofrozen, n_total_docs)\n",
    "        prior_nofrozen = rotateArray(prior_nofrozen, len(prior_nofrozen), own_topics)\n",
    "        doc_topics_all.append(doc_topics)\n",
    "        \n",
    "    # Step 3 - Document generation\n",
    "    documents_all = []\n",
    "    # z_all = []\n",
    "    \n",
    "    if gen_docs:\n",
    "        for i in np.arange(n_nodes):\n",
    "            print(\"Generating document words for node \", str(i))\n",
    "            documents = [] # Document words\n",
    "            #z = [] # Assignments\n",
    "            for docid in tqdm(np.arange(n_total_docs)):\n",
    "                doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
    "                this_doc_words = []\n",
    "                #this_doc_assigns = []\n",
    "                for wd_idx in np.arange(doc_len):\n",
    "\n",
    "                    tpc = np.nonzero(np.random.multinomial(1, doc_topics_all[i][docid]))[0][0]\n",
    "                    #this_doc_assigns.append(tpc)\n",
    "                    if alg == \"lda\":\n",
    "                        word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
    "                    else: #prodlda\n",
    "                        pval = np.power(topic_vectors[tpc], doc_topics_all[i][docid][tpc])\n",
    "                        weights = torch.tensor(pval, dtype=torch.float) # create a tensor of weights\n",
    "                        word = torch.multinomial(weights, 1).numpy()[0]\n",
    "                        #pval = normalize(pval[:,np.newaxis], norm='l1', axis=0).ravel()\n",
    "                        #word = np.nonzero(np.random.multinomial(1, b))[0][0]\n",
    "                    this_doc_words.append('wd'+str(word))\n",
    "                #z.append(this_doc_assigns)\n",
    "                documents.append(this_doc_words)\n",
    "            documents_all.append(documents)\n",
    "            #z_all.append(z)\n",
    "    \n",
    "    return topic_vectors, doc_topics_all, documents_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eecc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_topic_word_to_init_size(vocab_size, model, model_type,\n",
    "                                    ntopics, id2token, all_words):\n",
    "    \"\"\"It converts the topic-word distribution matrix obtained from the training of a model into a matrix with the dimensions of the original topic-word distribution, assigning zeros to those words that are not present in the corpus. \n",
    "    It is only of use in case we are training a model over a synthetic dataset, so as to later compare the performance of the attained model in what regards to the similarity between the original and the trained model.\n",
    "\n",
    "    Args:\n",
    "        * vocab_size (int):       Size of the synethic'data vocabulary.\n",
    "        * model (AVITM/CTM):      Model whose topic-word matrix is being transformed.\n",
    "        * model_type (str):       Type of the trained model (e.g. AVITM)\n",
    "        * ntopics (int):          Number of topics of the trained model.\n",
    "        * id2token (List[tuple]): Mappings with the content of the document-term matrix.\n",
    "        * all_words (List[str]):  List of all the words of the vocabulary of size vocab_size.\n",
    "\n",
    "    Returns:\n",
    "        * ndarray: Normalized transormed topic-word distribution.\n",
    "    \"\"\"\n",
    "    if model_type == \"avitm\":\n",
    "        w_t_distrib = np.zeros((ntopics, vocab_size), dtype=np.float64)\n",
    "        wd = model.get_topic_word_distribution()\n",
    "        for i in np.arange(ntopics):\n",
    "            for idx, word in id2token.items():\n",
    "                for j in np.arange(len(all_words)):\n",
    "                    if all_words[j] == word:\n",
    "                        w_t_distrib[i, j] = wd[i][idx]\n",
    "                        break\n",
    "        normalized_array = normalize(w_t_distrib,axis=1,norm='l1')\n",
    "        return normalized_array\n",
    "    else:\n",
    "        print(\"Method not impleemnted for the selected model type\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff459b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_betas(beta, topic_vectors):\n",
    "    print('TÃ³picos (equivalentes) evaluados correctamente:')\n",
    "    score = np.sum(np.max(np.sqrt(beta).dot(np.sqrt(topic_vectors.T)), axis=0))\n",
    "    printgr(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_thetas(thetas_theoretical, thetas_actual, n_docs):\n",
    "    sim_mat_theoretical = np.sqrt(thetas_theoretical).dot(np.sqrt(thetas_theoretical.T))\n",
    "    sim_mat_actual = np.sqrt(thetas_actual).dot(np.sqrt(thetas_actual.T))\n",
    "    print('Difference in evaluation of doc similarity:')\n",
    "    score = np.sum(np.abs(sim_mat_theoretical - sim_mat_actual))/n_docs\n",
    "    printgr(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4a97c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BOWDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Class to load BOW dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, idx2token, cv):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes BOWDataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape=(n_samples, n_features)\n",
    "            Document-term matrix\n",
    "        idx2token : list\n",
    "            A list of feature names\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X\n",
    "        self.idx2token = idx2token\n",
    "        self.cv = cv\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns length of dataset.\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Returns sample from dataset at index i.\"\"\"\n",
    "        X = torch.FloatTensor(self.X[i])\n",
    "\n",
    "        return {'X': X}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28701d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hold_out_dataset(hold_out_corpus, cv, idx2token):\n",
    "\n",
    "    docs_ho_conv = \\\n",
    "        [\" \".join(hold_out_corpus[i]) for i in np.arange(len(hold_out_corpus))]\n",
    "    ho_bow = cv.transform(docs_ho_conv)\n",
    "    ho_bow = ho_bow.toarray()\n",
    "    ho_data = BOWDataset(ho_bow, idx2token, cv)\n",
    "\n",
    "    return ho_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702129cc",
   "metadata": {},
   "source": [
    "## **1. Generation of documents**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b9a62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8287690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling settings\n",
    "vocab_size = 5000\n",
    "n_topics = 50\n",
    "beta = 1e-2\n",
    "alpha = 5/n_topics\n",
    "n_docs = 1000\n",
    "n_docs_inf = 1000\n",
    "n_docs_global_inf = 1000#int(n_docs / n_nodes)\n",
    "nwords = (150, 250) #Min and max lengths of the documents\n",
    "alg = \"lda\" #\"prod\"\n",
    "\n",
    "tm_settings = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"n_topics\": n_topics,\n",
    "    \"beta\": beta,\n",
    "    \"alpha\": alpha,\n",
    "    \"n_docs\": n_docs,\n",
    "    \"n_docs_inf\": n_docs_inf,\n",
    "    \"n_docs_global_inf\": n_docs_global_inf,\n",
    "    \"nwords\": nwords,\n",
    "    \"alg\": alg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bf29727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized settings\n",
    "\n",
    "frozen_topics = 5\n",
    "prior_frozen = frozen_topics * [alpha]\n",
    "own_topics = int((n_topics-frozen_topics)/n_nodes)\n",
    "prior_nofrozen = own_topics * [alpha] + (n_topics-frozen_topics-own_topics) * [alpha/10000]\n",
    "\n",
    "centralized_settings = {\n",
    "    \"n_nodes\": n_nodes,\n",
    "    \"frozen_topics\": frozen_topics,\n",
    "    \"prior_frozen\": prior_frozen,\n",
    "    \"own_topics\": own_topics,\n",
    "    \"prior_nofrozen\": prior_nofrozen\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40608d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating document words for node  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2000/2000 [00:48<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating document words for node  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2000/2000 [00:48<00:00, 41.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating document words for node  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2000/2000 [00:49<00:00, 40.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating document words for node  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2000/2000 [00:49<00:00, 40.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating document words for node  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2000/2000 [00:49<00:00, 40.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate documents\n",
    "topic_vectors, doc_topics_all, documents_all = generateSynthetic(False, True, **tm_settings, **centralized_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639ce1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bow_text</th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wd4385 wd4657 wd1715 wd4656 wd4827 wd3446 wd26...</td>\n",
       "      <td>computer_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wd4110 wd205 wd4781 wd3873 wd559 wd2228 wd559 ...</td>\n",
       "      <td>computer_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wd2621 wd2717 wd3522 wd1996 wd1110 wd1294 wd94...</td>\n",
       "      <td>computer_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wd3304 wd4907 wd1543 wd3278 wd3388 wd665 wd366...</td>\n",
       "      <td>computer_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wd963 wd4973 wd2901 wd1363 wd2980 wd2980 wd413...</td>\n",
       "      <td>computer_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>wd378 wd1803 wd1043 wd3308 wd3432 wd3357 wd360...</td>\n",
       "      <td>political_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>wd2374 wd1786 wd3060 wd2088 wd328 wd3849 wd253...</td>\n",
       "      <td>political_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>wd2730 wd4500 wd1326 wd3772 wd4569 wd4012 wd55...</td>\n",
       "      <td>political_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>wd3153 wd29 wd1188 wd2964 wd2911 wd205 wd3185 ...</td>\n",
       "      <td>political_science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>wd3107 wd3492 wd1845 wd3489 wd4672 wd3810 wd11...</td>\n",
       "      <td>political_science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              bow_text      fieldsOfStudy\n",
       "0    wd4385 wd4657 wd1715 wd4656 wd4827 wd3446 wd26...   computer_science\n",
       "1    wd4110 wd205 wd4781 wd3873 wd559 wd2228 wd559 ...   computer_science\n",
       "2    wd2621 wd2717 wd3522 wd1996 wd1110 wd1294 wd94...   computer_science\n",
       "3    wd3304 wd4907 wd1543 wd3278 wd3388 wd665 wd366...   computer_science\n",
       "4    wd963 wd4973 wd2901 wd1363 wd2980 wd2980 wd413...   computer_science\n",
       "..                                                 ...                ...\n",
       "995  wd378 wd1803 wd1043 wd3308 wd3432 wd3357 wd360...  political_science\n",
       "996  wd2374 wd1786 wd3060 wd2088 wd328 wd3849 wd253...  political_science\n",
       "997  wd2730 wd4500 wd1326 wd3772 wd4569 wd4012 wd55...  political_science\n",
       "998  wd3153 wd29 wd1188 wd2964 wd2911 wd205 wd3185 ...  political_science\n",
       "999  wd3107 wd3492 wd1845 wd3489 wd4672 wd3810 wd11...  political_science\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [' '.join(doc) for doc in documents_all[0][0:n_docs]]\n",
    "df0 = pd.DataFrame(docs)\n",
    "df0.columns = [\"bow_text\"]\n",
    "df0['fieldsOfStudy'] = [\"computer_science\"] * len(df0)\n",
    "\n",
    "docs = [' '.join(doc) for doc in documents_all[1][0:n_docs]]\n",
    "df1 = pd.DataFrame(docs)\n",
    "df1.columns = [\"bow_text\"]\n",
    "df1['fieldsOfStudy'] = [\"economics\"] * len(df1)\n",
    "\n",
    "docs = [' '.join(doc) for doc in documents_all[2][0:n_docs]]\n",
    "df2 = pd.DataFrame(docs)\n",
    "df2.columns = [\"bow_text\"]\n",
    "df2['fieldsOfStudy'] = [\"sociology\"] * len(df2)\n",
    "\n",
    "docs = [' '.join(doc) for doc in documents_all[3][0:n_docs]]\n",
    "df3 = pd.DataFrame(docs)\n",
    "df3.columns = [\"bow_text\"]\n",
    "df3['fieldsOfStudy'] = [\"philosophy\"] * len(df3)\n",
    "\n",
    "docs = [' '.join(doc) for doc in documents_all[4][0:n_docs]]\n",
    "df4 = pd.DataFrame(docs)\n",
    "df4.columns = [\"bow_text\"]\n",
    "df4['fieldsOfStudy'] = [\"political_science\"] * len(df4)\n",
    "\n",
    "df = pd.concat([df0, df1, df2, df3, df4])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81031d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the inference corpus  5000\n",
      "Shape of inf_doc_topics (5000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Generate inference corpus and its docs_topics\n",
    "inf = [doc for docs_node in documents_all for doc in docs_node[n_docs:(n_docs+n_docs_global_inf)]]\n",
    "print(\"Length of the inference corpus \", str(len(inf)))\n",
    "\n",
    "for i in range(len(doc_topics_all)):\n",
    "    if i == 0:\n",
    "        inf_doc_topics = doc_topics_all[i][n_docs:(n_docs+n_docs_global_inf)]\n",
    "    else:\n",
    "        inf_doc_topics = np.concatenate((inf_doc_topics,doc_topics_all[i][n_docs:(n_docs+n_docs_global_inf)])) \n",
    "print(\"Shape of inf_doc_topics\", str(inf_doc_topics.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2881a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training dataset in file\n",
    "filepath = Path('data/training_data/out.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfd9ae",
   "metadata": {},
   "source": [
    "## **2. Evaluation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5953b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [00:38,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10225347 0.08972973 0.11785859 ... 0.13537964 0.11281251 0.07142649]\n",
      " [0.08366234 0.10211106 0.03991462 ... 0.01728298 0.09018794 0.0590278 ]\n",
      " [0.07233718 0.09996512 0.10105768 ... 0.11944315 0.14564925 0.09656231]\n",
      " ...\n",
      " [0.11041382 0.14063583 0.08788011 ... 0.10886418 0.08803174 0.08655686]\n",
      " [0.10145532 0.07431257 0.09304321 ... 0.04613498 0.13449026 0.06671481]\n",
      " [0.11221697 0.10069423 0.08394611 ... 0.02899571 0.1286096  0.08313668]]\n",
      "Difference in evaluation of doc similarity:\n",
      "\u001b[38;5;2m4111.595927833973\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [01:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1010623  0.08040764 0.10750438 ... 0.12043143 0.12487704 0.07974143]\n",
      " [0.1126292  0.1027429  0.10769147 ... 0.07497612 0.0860968  0.09462127]\n",
      " [0.08610949 0.07778461 0.08552249 ... 0.11447899 0.15131176 0.12126468]\n",
      " ...\n",
      " [0.20180932 0.0915784  0.10235601 ... 0.06723996 0.08419517 0.0881216 ]\n",
      " [0.18810379 0.06488642 0.10371054 ... 0.07440621 0.07720394 0.09226294]\n",
      " [0.16082843 0.06065461 0.10503435 ... 0.07018349 0.07126443 0.08907101]]\n",
      "Difference in evaluation of doc similarity:\n",
      "\u001b[38;5;2m4196.128134706327\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [00:40,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04823256 0.17635389 0.08902908 ... 0.0459877  0.14566489 0.21372318]\n",
      " [0.05765652 0.05504364 0.10553588 ... 0.07237964 0.05265043 0.08393895]\n",
      " [0.04832333 0.08013556 0.14371341 ... 0.0861653  0.09775459 0.12772186]\n",
      " ...\n",
      " [0.10762317 0.06059394 0.08227239 ... 0.07313255 0.09156939 0.19718215]\n",
      " [0.05585344 0.08707504 0.19382078 ... 0.18452522 0.05765669 0.18174644]\n",
      " [0.0607813  0.03585553 0.09391747 ... 0.09457948 0.05914036 0.0804162 ]]\n",
      "Difference in evaluation of doc similarity:\n",
      "\u001b[38;5;2m4131.990892491831\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [00:40,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0971758  0.10062097 0.09833107 ... 0.15578528 0.06597111 0.11631232]\n",
      " [0.0819947  0.05237578 0.06023697 ... 0.14115332 0.09656935 0.1012709 ]\n",
      " [0.09813085 0.09338076 0.13665884 ... 0.08573293 0.07618148 0.11354846]\n",
      " ...\n",
      " [0.09003014 0.11010802 0.09959604 ... 0.11332934 0.07839242 0.11658286]\n",
      " [0.10488924 0.12911632 0.11506834 ... 0.10177713 0.07389606 0.09492969]\n",
      " [0.08649923 0.08551789 0.04947664 ... 0.10318171 0.09594417 0.07254157]]\n",
      "Difference in evaluation of doc similarity:\n",
      "\u001b[38;5;2m4144.742258359256\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [00:40,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09949548 0.10819748 0.13640513 ... 0.07729032 0.13251615 0.13443898]\n",
      " [0.10015063 0.21524402 0.05697452 ... 0.05780128 0.03372726 0.11331372]\n",
      " [0.0718631  0.11162189 0.09783238 ... 0.08127696 0.09580944 0.12422001]\n",
      " ...\n",
      " [0.11695234 0.09008134 0.07174911 ... 0.10950576 0.15367256 0.09825709]\n",
      " [0.09501    0.15423849 0.05818317 ... 0.07234916 0.15165965 0.1026371 ]\n",
      " [0.12493182 0.13098422 0.06291202 ... 0.09708492 0.05506542 0.07703925]]\n",
      "Difference in evaluation of doc similarity:\n",
      "\u001b[38;5;2m4139.749366920617\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [20/20]: : 20it [00:40,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08542518 0.07607844 0.09754066 ... 0.19401567 0.08479702 0.09454972]\n",
      " [0.03905668 0.02452578 0.02728593 ... 0.32572629 0.22696612 0.05531969]\n",
      " [0.08506558 0.06716078 0.09140845 ... 0.09611458 0.25881431 0.07281016]\n",
      " ...\n",
      " [0.08194279 0.05790422 0.05936885 ... 0.16132596 0.13182935 0.13840428]\n",
      " [0.10423137 0.07257192 0.06151288 ... 0.12071347 0.11253575 0.16314245]\n",
      " [0.07001191 0.04233691 0.05205844 ... 0.29163649 0.14413459 0.06231043]]\n",
      "Difference in evaluation of doc similarity:\n",
      "\u001b[38;5;2m4157.72458740711\u001b[0m\n",
      "Difference in evaluation of doc similarity:\n",
      "\u001b[38;5;2m761.4294931685141\u001b[0m\n",
      "####################################################\n",
      "Averages of inferred thetas in nodes (level-1 models):  4137.160606602558\n",
      "Inferred thetas with second level model:  4196.128134706327\n",
      "Baseline:  761.4294931685141\n"
     ]
    }
   ],
   "source": [
    "path_output = Path('/export/usuarios_ml4ds/lbartolome/2hNTM/data/output')\n",
    "\n",
    "sim_thetas_nodes = []\n",
    "for entry in path_output.iterdir():\n",
    "    path_model = entry.joinpath(\"modelFiles/model.pickle\")\n",
    "    \n",
    "    # Get model object to perform inference\n",
    "    avitm = unpickler(path_model)\n",
    "    \n",
    "    # Get inf corpus in avitm format\n",
    "    ho_data = prepare_hold_out_dataset(\n",
    "            inf, avitm.train_data.cv, avitm.train_data.idx2token)\n",
    "    \n",
    "    thetas_inf = np.asarray(avitm.get_doc_topic_distribution(ho_data))\n",
    "    thetas_theoretical = inf_doc_topics\n",
    "\n",
    "    # Eval thetas\n",
    "    thetas_sim = eval_thetas(thetas_theoretical, thetas_inf, len(thetas_inf))\n",
    "    \n",
    "    if entry.as_posix().split(\"/\")[-1].startswith(\"node\"):\n",
    "        sim_thetas_nodes.append(thetas_sim)\n",
    "    else:\n",
    "        sim_thetas_centralized = thetas_sim\n",
    "\n",
    "avg_sim_thetas_nodes = sum(sim_thetas_nodes)/n_nodes\n",
    "\n",
    "# Baseline\n",
    "# Baseline doc-topics generation\n",
    "topic_vectors, doc_topics_all, _ = generateSynthetic(True, False, **tm_settings, **centralized_settings)\n",
    "\n",
    "for i in range(len(doc_topics_all)):\n",
    "    if i == 0:\n",
    "        thetas_bas = doc_topics_all[i]\n",
    "    else:\n",
    "        thetas_bas = np.concatenate((thetas_bas,doc_topics_all[i]))\n",
    "        \n",
    "thetas_theoretical = inf_doc_topics\n",
    "thetas_baseline = eval_thetas(thetas_theoretical, thetas_bas, len(thetas_bas))\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(\"Averages of inferred thetas in nodes (level-1 models): \", avg_sim_thetas_nodes)\n",
    "print(\"Inferred thetas with second level model: \", sim_thetas_centralized)\n",
    "print(\"Baseline: \", thetas_baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
